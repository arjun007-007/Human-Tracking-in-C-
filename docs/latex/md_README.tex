 \href{https://codecov.io/gh/MayankD409/Human_Tracking_CPP}{\texttt{ }} \mbox{[}\mbox{]}(LICENSE)

This repository consists of deliverables for the Midterm Project for,
\begin{DoxyItemize}
\item Lowell Lobo (120095719)
\item Kautilya Chappidi (120380204)
\item Mayank Deshpande (120387333)
\end{DoxyItemize}\hypertarget{md_README_autotoc_md0}{}\doxysection{Project Overview}\label{md_README_autotoc_md0}
Perception is essential for object detection, environmental awareness, path planning and control; through perception, a system can truly be considered autonomous. Acme requires the development of a perception component in its autonomous car system, and a human obstacle detection and tracking module needs to be built. The aim is to build a perception module to detect human objects and track their location directly into the robot reference frame. The location will first be found with respect to the camera\textquotesingle{}s reference frame and then using a transformation, the location with respect to the robot reference frame will be found. The module is built such that, the system that implements it will consists of a camera mounted on a car that will keep track of human motion within the cameraâ€™s field of view. But the module will be unable to handle occlusion cases.\hypertarget{md_README_autotoc_md1}{}\doxysubsection{Assumption}\label{md_README_autotoc_md1}
The camera used for perception will be placed on the top of the car. The camera reference frame is taken such that the z-\/axis is pointing out of the lens, the x-\/axis is to the right, and the y-\/axis is pointing downward. The robot reference frame is at the centre of mass where the z-\/axis points upward, and the x-\/axis is to the right, and the y-\/axis points straight ahead. We assume the camera is monocular, and the user must input most of the configuration information\hypertarget{md_README_autotoc_md2}{}\doxysubsection{Features}\label{md_README_autotoc_md2}

\begin{DoxyItemize}
\item Detection of human obstacles and tracking using monocular camera
\item Location of human obstacle with respect to robot reference frame
\item Display\+Class to implement and test the module
\end{DoxyItemize}\hypertarget{md_README_autotoc_md3}{}\doxysubsection{Deliverables}\label{md_README_autotoc_md3}

\begin{DoxyItemize}
\item C++ library/\+API for human obstacle and tracking
\item Git\+Hub Repository with CI and Code\+Cov
\item UML and Dependency Diagrams
\item Doxygen Documentation
\end{DoxyItemize}\hypertarget{md_README_autotoc_md4}{}\doxysubsection{Constraints}\label{md_README_autotoc_md4}
The module uses a monocular camera and is unable to handle occlusions. Also, the monocular camera cannot perform precise depth calculation and thus would require a deep learning model for accurate estimation. The runtime fps and memory management depend on the physical constraints of the system.\hypertarget{md_README_autotoc_md5}{}\doxysection{Personnel}\label{md_README_autotoc_md5}
Kautilya Chappiddi\+:-\/ ~\newline
 Graduate Student in Enginnering -\/ Robotics at University of Maryland, College Park

Lowell Lobo\+:-\/ ~\newline
 Graduate Student in Enginnering -\/ Robotics at University of Maryland, College Park

Mayank Deshpande\+:-\/ ~\newline
 Graduate Student in Enginnering -\/ Robotics at University of Maryland, College Park\hypertarget{md_README_autotoc_md6}{}\doxysection{Process}\label{md_README_autotoc_md6}
The project development will be executed using pair programming concepts. For all specific tasks, driver and navigator roles will be swapped. The project will follow AIP concepts and be performed using Pair Programming. The module works such that a video is fed into the system using the monocular camera. The program then converts the video stream into image frames and performs human obstacle detection. Once obstacles are found in the image frame, unique IDs are assigned to the objects, which will hold the calculated current location with respect to the robot reference frame. The process is repeated over time for tracking of obstacle movement. Testing of components is performed using Google\+Test, and system testing will be performed every iteration for overall functionality verification.\hypertarget{md_README_autotoc_md7}{}\doxysection{AIP Methodology}\label{md_README_autotoc_md7}
The project was implemented using Agile Iterative Process (AIP) along with pair programming and a test-\/driven development approach. Product backlog, iteration backlogs for each phase and work log have been created. Each iteration started with a meeting to discuss the agenda and ends with a sprint review.\hypertarget{md_README_autotoc_md8}{}\doxysection{Links}\label{md_README_autotoc_md8}
\href{https://docs.google.com/spreadsheets/d/17q5Q-qL-ZU2LQK8llQSjsIJ8_037MAFyNenNofpY_PI/edit\#gid=0}{\texttt{ AIP Google Sheet}}

\href{https://docs.google.com/document/d/1UX6oGidNdux2FeOGgIjD-ivvmi8oCiHlf1OJARla3Ro/edit}{\texttt{ Sprint Notes}}\hypertarget{md_README_autotoc_md9}{}\doxysection{Dependencies}\label{md_README_autotoc_md9}
The project uses the Open\+CV library to read video frame, perform image processing and run inference. ~\newline
 Together with Open\+CV, a pretrained Res\+Net Caffemodel is used in the implementation.\hypertarget{md_README_autotoc_md10}{}\doxysection{Installing Dependencies}\label{md_README_autotoc_md10}
Open\+CV can be installed as follows, 
\begin{DoxyCode}{0}
\DoxyCodeLine{sudo apt install libopencv-\/dev}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md11}{}\doxysection{API Information}\label{md_README_autotoc_md11}
The API contains three libraries, ~\newline
\hypertarget{md_README_autotoc_md12}{}\doxysubsubsection{1 -\/ detection library}\label{md_README_autotoc_md12}
~\newline
 The library is used to initialise and access the Video Stream and runs face detection to find human obstacles in the image frame.

It consists of two method and a constructor\+:-\/ ~\newline

\begin{DoxyItemize}
\item init\+Video\+Stream()
\item detect\+Faces() ~\newline
 -- Constructor ~\newline
 The constructor initialises the detection model in Open\+CV using the read\+Net() method ~\newline
 ~\newline
 -- init\+Video\+Stream() ~\newline
 The Video\+Stream is initialised to constantly read image frames from the camera setup. ~\newline
 ~\newline
 -- detect\+Faces() Uses the model that was initialised to detect faces in the image frame and return bounding boxes above a confidence level in the format cv\+::\+Rect ~\newline
 ~\newline

\end{DoxyItemize}\hypertarget{md_README_autotoc_md13}{}\doxysubsubsection{2 -\/ tracking library}\label{md_README_autotoc_md13}
~\newline
 The library is used to find the (x, y, z) coordinates of the obstacle with respect to the robot reference frame. The library also tracks the motion of obstacles by assigning a unique ID to each object.

It consists of four methods\+:-\/ ~\newline

\begin{DoxyItemize}
\item assign\+IDAnd\+Track()
\item dist\+From\+Camera()
\item dist\+From\+Car()
\item find\+Depth() ~\newline
 -- assign\+IDAnd\+Track() ~\newline
 This method receives the bounding boxes and returns the bounding boxes with a unique ID assigned to them. If there are no IDs assigned, the algorithm assigns IDs to each detection. If the number of detections are equal to the number of IDs assigned, the algorithm computes and assigns IDs based on minimum Euclidean distance between detection. This consists the tracking feature. If the number of detections are greater than the number of assigned IDs, then the algorithm reassigns the correct IDs to the corresponding detection and assigns new IDs to the new detections. If the number of detections are lesser than the number of assigned IDs, if the detection is near the edge and the minimum Euclidean distance is greater than a specific value then the ID is deallocated. But if the detection is lost in the center of the frame, false detection is assumed, and the detection with ID is retained. If all conditions above are not met, the ID is reassigned. ~\newline
 ~\newline
 -- dist\+From\+Camera() ~\newline
 This method takes in the the IDs mapped to the bounding boxes and returns the (x, y, z) pixel distance of the obstacle from the camera reference frame using geometry. ~\newline
 ~\newline
 -- dist\+From\+Car() ~\newline
 This method takes in (x, y, z) pixel distance of the obstacle from the camera reference frame and returns the (x, y, z) distance of the obstacle from the car reference frame in inches. Rotation, Translation and geometry are used to find the distance in inches. ~\newline
 ~\newline
 -- find\+Depth() ~\newline
 This method takes in the ID and returns the z distance in inches. The z distance is found analytically by using sampling and linearising the conversion function.
\end{DoxyItemize}\hypertarget{md_README_autotoc_md14}{}\doxysection{Running the Application}\label{md_README_autotoc_md14}
\hypertarget{md_README_autotoc_md15}{}\doxysubsection{Build via command-\/line}\label{md_README_autotoc_md15}

\begin{DoxyCode}{0}
\DoxyCodeLine{\# Configure the project and generate a native build system:}
\DoxyCodeLine{  \# Must re-\/run this command whenever any CMakeLists.txt file has been changed.}
\DoxyCodeLine{  cmake -\/S ./ -\/B build/}
\DoxyCodeLine{}
\DoxyCodeLine{\# Compile and build the project:}
\DoxyCodeLine{  \# rebuild only files that are modified since the last build}
\DoxyCodeLine{  cmake -\/-\/build build/}
\DoxyCodeLine{  \# or rebuild everything from scracth}
\DoxyCodeLine{  cmake -\/-\/build build/ -\/-\/clean-\/first}
\DoxyCodeLine{  \# to see verbose output, do:}
\DoxyCodeLine{  cmake -\/-\/build build/ -\/-\/verbose}
\DoxyCodeLine{}
\DoxyCodeLine{\# Alternate method to build the project:}
\DoxyCodeLine{  \# build compile\_commands.json from scratch}
\DoxyCodeLine{  bear -\/-\/ cmake -\/-\/build build/ -\/-\/clean-\/first}
\DoxyCodeLine{  \# or, update the existing compile\_commands.json}
\DoxyCodeLine{  bear -\/-\/append -\/-\/ cmake -\/-\/build build/}
\DoxyCodeLine{}
\DoxyCodeLine{\# Run program for testing the actual application:}
\DoxyCodeLine{  ./build/app/human-\/tracker}
\DoxyCodeLine{}
\DoxyCodeLine{\# Clean}
\DoxyCodeLine{  cmake -\/-\/build build/ -\/-\/target clean}
\DoxyCodeLine{\# Clean and start over:}
\DoxyCodeLine{  rm -\/rf build/}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md16}{}\doxysubsection{Running Unit Tests}\label{md_README_autotoc_md16}

\begin{DoxyCode}{0}
\DoxyCodeLine{\# Run tests:}
\DoxyCodeLine{  cd build/; ctest; cd -\/}
\DoxyCodeLine{  \# or if you have newer cmake}
\DoxyCodeLine{  ctest -\/-\/test-\/dir build/}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md17}{}\doxysubsection{Generate Documentation}\label{md_README_autotoc_md17}
\hypertarget{md_README_autotoc_md18}{}\doxysubsubsection{Method 1}\label{md_README_autotoc_md18}

\begin{DoxyCode}{0}
\DoxyCodeLine{\# Download doxygen}
\DoxyCodeLine{  sudo apt-\/get install doxygen}
\DoxyCodeLine{\# Build documentation}
\DoxyCodeLine{  doxygen dconfig}

\end{DoxyCode}
 \hypertarget{md_README_autotoc_md19}{}\doxysubsubsection{Method 2}\label{md_README_autotoc_md19}

\begin{DoxyCode}{0}
\DoxyCodeLine{\# Build docs:}
\DoxyCodeLine{  cmake -\/-\/build build/ -\/-\/target docs}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md20}{}\doxysubsubsection{View Documentation}\label{md_README_autotoc_md20}

\begin{DoxyCode}{0}
\DoxyCodeLine{\# open a web browser to browse the doc}
\DoxyCodeLine{  open docs/html/index.html}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md21}{}\doxysection{Building for code coverage}\label{md_README_autotoc_md21}

\begin{DoxyCode}{0}
\DoxyCodeLine{\# if you don't have gcovr or lcov installed, do:}
\DoxyCodeLine{  sudo apt-\/get install gcovr lcov}
\DoxyCodeLine{\# Set the build type to Debug and WANT\_COVERAGE=ON}
\DoxyCodeLine{  cmake -\/D WANT\_COVERAGE=ON -\/D CMAKE\_BUILD\_TYPE=Debug -\/S ./ -\/B build/}
\DoxyCodeLine{\# Now, do a clean compile, run unit test, and generate the covereage report}
\DoxyCodeLine{  cmake -\/-\/build build/ -\/-\/clean-\/first -\/-\/target all test\_coverage}
\DoxyCodeLine{\# open a web browser to browse the test coverage report}
\DoxyCodeLine{  open build/test\_coverage/index.html}
\DoxyCodeLine{}
\DoxyCodeLine{This generates a index.html page in the build/test\_coverage sub-\/directory that can be viewed locally in a web browser.}

\end{DoxyCode}


You can also get code coverage report for the {\itshape human-\/tracker} target, instead of unit test. Repeat the previous 2 steps but with the {\itshape app\+\_\+coverage} target\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\# Now, do another clean compile, run shell-\/app, and generate its covereage report}
\DoxyCodeLine{  cmake -\/-\/build build/ -\/-\/clean-\/first -\/-\/target all app\_coverage}
\DoxyCodeLine{\# open a web browser to browse the test coverage report}
\DoxyCodeLine{  open build/app\_coverage/index.html}
\DoxyCodeLine{}
\DoxyCodeLine{This generates a index.html page in the build/app\_coverage sub-\/directory that can be viewed locally in a web browser.}

\end{DoxyCode}
\hypertarget{md_README_autotoc_md22}{}\doxysection{Code\+Cov Outputs}\label{md_README_autotoc_md22}
Unit tests were chosen to try to meet all run condition cases, but some were still missed. Thus coverage report is reduced in \mbox{\hyperlink{classTrackingClass}{Tracking\+Class}}. ~\newline
 \mbox{\hyperlink{classDetectionClass}{Detection\+Class}} requires to initialise a Video\+Stream, which is not possible in Git\+Hub Actions.\hypertarget{md_README_autotoc_md23}{}\doxysubsection{compile\+\_\+commands.\+json Errors}\label{md_README_autotoc_md23}
In case of error regarding the generation of compile\+\_\+commands.\+json file, Delete the compile\+\_\+commands.\+json file and run the following 
\begin{DoxyCode}{0}
\DoxyCodeLine{bear -\/-\/ cmake -\/-\/build build/ -\/-\/clean-\/first}

\end{DoxyCode}
 